{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python\n",
    "#pip install tensorflow\n",
    "#pip install tensorflow_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import string\n",
    "import re\n",
    "import subprocess\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import collections\n",
    "import unicodedata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.utils.layer_utils import count_params\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow Hub\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# # Colab auth\n",
    "# from google.colab import auth\n",
    "# from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to implement trainign , we will need to obtain the following:\n",
    "- 1. The training data : images of different images to train on\n",
    "- 2. The model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.14.0\n",
      "Eager Execution Enabled: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of replicas: 1\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[]\n",
      "GPU Available:  []\n",
      "All Physical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately,\n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(packet_url, base_path=\"\", extract=False, headers=None):\n",
    "  if base_path != \"\":\n",
    "    if not os.path.exists(base_path):\n",
    "      os.mkdir(base_path)\n",
    "  packet_file = os.path.basename(packet_url)\n",
    "  with requests.get(packet_url, stream=True, headers=headers) as r:\n",
    "      r.raise_for_status()\n",
    "      with open(os.path.join(base_path,packet_file), 'wb') as f:\n",
    "          for chunk in r.iter_content(chunk_size=8192):\n",
    "              f.write(chunk)\n",
    "\n",
    "  if extract:\n",
    "    if packet_file.endswith(\".zip\"):\n",
    "      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n",
    "        zfile.extractall(base_path)\n",
    "    else:\n",
    "      packet_name = packet_file.split('.')[0]\n",
    "      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n",
    "        tfile.extractall(base_path)\n",
    "\n",
    "def compute_dataset_metrics(data_list):\n",
    "  data_list_with_metrics = []\n",
    "  for item in data_list:\n",
    "    # Read image\n",
    "    image = cv2.imread(item[1])\n",
    "    data_list_with_metrics.append((item[0],item[1],image.shape[0],image.shape[1],image.nbytes / (1024 * 1024.0)))\n",
    "\n",
    "  # Build a dataframe\n",
    "  data_list_with_metrics = np.asarray(data_list_with_metrics)\n",
    "  dataset_df = pd.DataFrame({\n",
    "    'label': data_list_with_metrics[:, 0],\n",
    "    'path': data_list_with_metrics[:, 1],\n",
    "    'height': data_list_with_metrics[:, 2],\n",
    "    'width': data_list_with_metrics[:, 3],\n",
    "    'size': data_list_with_metrics[:, 4],\n",
    "    })\n",
    "\n",
    "  dataset_df[\"height\"] = dataset_df[\"height\"].astype(int)\n",
    "  dataset_df[\"width\"] = dataset_df[\"width\"].astype(int)\n",
    "  dataset_df[\"size\"] = dataset_df[\"size\"].astype(float)\n",
    "\n",
    "  dataset_mem_size = dataset_df[\"size\"].sum()\n",
    "  value_counts = dataset_df[\"label\"].value_counts()\n",
    "  height_details = dataset_df[\"height\"].describe()\n",
    "  width_details = dataset_df[\"width\"].describe()\n",
    "\n",
    "  print(\"Dataset Metrics:\")\n",
    "  print(\"----------------\")\n",
    "  print(\"Label Counts:\")\n",
    "  print(value_counts)\n",
    "  print(\"Image Width:\")\n",
    "  print(\"Min:\",width_details[\"min\"],\" Max:\",width_details[\"max\"])\n",
    "  print(\"Image Height:\")\n",
    "  print(\"Min:\",height_details[\"min\"],\" Max:\",height_details[\"max\"])\n",
    "  print(\"Size in memory:\",round(dataset_df[\"size\"].sum(),2),\"MB\")\n",
    "\n",
    "class JsonEncoder(json.JSONEncoder):\n",
    "  def default(self, obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, decimal.Decimal):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return super(JsonEncoder, self).default(obj)\n",
    "\n",
    "experiment_name = None\n",
    "def create_experiment():\n",
    "  global experiment_name\n",
    "  experiment_name = \"experiment_\" + str(int(time.time()))\n",
    "\n",
    "  # Create experiment folder\n",
    "  if not os.path.exists(experiment_name):\n",
    "      os.mkdir(experiment_name)\n",
    "\n",
    "def save_data_details(data_details):\n",
    "  with open(os.path.join(experiment_name,\"data_details.json\"), \"w\") as json_file:\n",
    "    json_file.write(json.dumps(data_details,cls=JsonEncoder))\n",
    "\n",
    "def save_model(model,model_name=\"model01\"):\n",
    "\n",
    "  # Save the enitire model (structure + weights)\n",
    "  model.save(os.path.join(experiment_name,model_name+\".keras\"))\n",
    "\n",
    "  # Save only the weights\n",
    "  model.save_weights(os.path.join(experiment_name,model_name+\".h5\"))\n",
    "\n",
    "  # Save the structure only\n",
    "  model_json = model.to_json()\n",
    "  with open(os.path.join(experiment_name,model_name+\".json\"), \"w\") as json_file:\n",
    "      json_file.write(model_json)\n",
    "\n",
    "def get_model_size(model_name=\"model01\"):\n",
    "  model_size = os.stat(os.path.join(experiment_name,model_name+\".h5\")).st_size\n",
    "  return model_size\n",
    "\n",
    "def evaluate_save_model(model,test_data, model_train_history,execution_time, learning_rate, batch_size, epochs, optimizer,save=True):\n",
    "\n",
    "  # Get the number of epochs the training was run for\n",
    "  num_epochs = len(model_train_history[\"loss\"])\n",
    "\n",
    "  # Plot training results\n",
    "  fig = plt.figure(figsize=(15,5))\n",
    "  axs = fig.add_subplot(1,2,1)\n",
    "  axs.set_title('Loss')\n",
    "  # Plot all metrics\n",
    "  for metric in [\"loss\",\"val_loss\"]:\n",
    "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
    "  axs.legend()\n",
    "\n",
    "  axs = fig.add_subplot(1,2,2)\n",
    "  axs.set_title('Accuracy')\n",
    "  # Plot all metrics\n",
    "  for metric in [\"accuracy\",\"val_accuracy\"]:\n",
    "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
    "  axs.legend()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  # Evaluate on test data\n",
    "  evaluation_results = model.evaluate(test_data)\n",
    "  print(evaluation_results)\n",
    "\n",
    "  if save:\n",
    "    # Save model\n",
    "    save_model(model, model_name=model.name)\n",
    "    model_size = get_model_size(model_name=model.name)\n",
    "\n",
    "    # Save model history\n",
    "    with open(os.path.join(experiment_name,model.name+\"_train_history.json\"), \"w\") as json_file:\n",
    "        json_file.write(json.dumps(model_train_history,cls=JsonEncoder))\n",
    "\n",
    "    trainable_parameters = count_params(model.trainable_weights)\n",
    "    non_trainable_parameters = count_params(model.non_trainable_weights)\n",
    "\n",
    "    # Save model metrics\n",
    "    metrics ={\n",
    "        \"trainable_parameters\":trainable_parameters,\n",
    "        \"execution_time\":execution_time,\n",
    "        \"loss\":evaluation_results[0],\n",
    "        \"accuracy\":evaluation_results[1],\n",
    "        \"model_size\":model_size,\n",
    "        \"learning_rate\":learning_rate,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"epochs\":epochs,\n",
    "        \"optimizer\":type(optimizer).__name__\n",
    "    }\n",
    "    with open(os.path.join(experiment_name,model.name+\"_model_metrics.json\"), \"w\") as json_file:\n",
    "        json_file.write(json.dumps(metrics,cls=JsonEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
