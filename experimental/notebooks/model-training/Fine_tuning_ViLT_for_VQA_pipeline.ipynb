{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e403a7a8-5c8b-422e-bdff-9cf501c03471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Optional\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from transformers import ViltConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from transformers import ViltProcessor\n",
    "import numpy as np\n",
    "from transformers import ViltForQuestionAnswering\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d392ba8-0803-4099-8cf7-68fa6f796315",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a657652e-4d5c-4108-a2d2-0973d30fd2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '/Users/samch/OneDrive/Desktop/FALL 2023/AC215/AC215-BiteSize/notebooks/vaq_files/file_names/val2014'\n",
    "# path_questions = '/Users/samch/OneDrive/Desktop/FALL 2023/AC215/AC215-BiteSize/notebooks/vaq_files/datasets/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "# path_annotations = '/Users/samch/OneDrive/Desktop/FALL 2023/AC215/AC215-BiteSize/notebooks/vaq_files/data_vqa/v2_mscoco_val2014_annotations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bebc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/kimberlyllajarunaperalta/Documents/MLOps/vaq_files/data_vqa/val2014'\n",
    "path_questions = '/Users/kimberlyllajarunaperalta/Documents/MLOps/vaq_files/data_vqa/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "path_annotations = '/Users/kimberlyllajarunaperalta/Documents/MLOps/vaq_files/data_vqa/v2_mscoco_val2014_annotations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d530c3c6-3724-47d6-94da-7fd0a4a34455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_from_filename(filename: str) -> Optional[int]:\n",
    "    match = filename_re.fullmatch(filename)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return int(match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1769255-5ee5-4486-a748-f0729e3545a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  input_ids = [item['input_ids'] for item in batch]\n",
    "  pixel_values = [item['pixel_values'] for item in batch]\n",
    "  attention_mask = [item['attention_mask'] for item in batch]\n",
    "  token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "  labels = [item['labels'] for item in batch]\n",
    "\n",
    "  # create padded pixel values and corresponding pixel mask\n",
    "  encoding = processor.image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "  # create new batch\n",
    "  batch = {}\n",
    "  batch['input_ids'] = torch.stack(input_ids)\n",
    "  batch['attention_mask'] = torch.stack(attention_mask)\n",
    "  batch['token_type_ids'] = torch.stack(token_type_ids)\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = torch.stack(labels)\n",
    "\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6351fe-b97d-4386-8c78-574d1e70a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(count: int) -> float:\n",
    "    return min(1.0, count / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27a0228-85a5-4460-a2ad-e1ae2413ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA (v2) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, questions, annotations, processor):\n",
    "        self.questions = questions\n",
    "        self.annotations = annotations\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get image + text\n",
    "        annotation = self.annotations[idx]\n",
    "        questions = self.questions[idx]\n",
    "        image = Image.open(id_to_filename[annotation['image_id']])\n",
    "        text = questions['question']\n",
    "\n",
    "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "          encoding[k] = v.squeeze()\n",
    "        # add labels\n",
    "        labels = annotation['labels']\n",
    "        scores = annotation['scores']\n",
    "        # based on: https://github.com/dandelin/ViLT/blob/762fd3975c180db6fc88f577cf39549983fa373a/vilt/modules/objectives.py#L301\n",
    "        targets = torch.zeros(len(config.id2label))\n",
    "        for label, score in zip(labels, scores):\n",
    "              targets[label] = score\n",
    "        encoding[\"labels\"] = targets\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d14567-4010-417f-8212-c7f676b85478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc20a9bdcff4df983101a81409692c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_names = [f for f in tqdm(listdir(root)) if isfile(join(root, f))]\n",
    "filename_re = re.compile(r\".*(\\d{12})\\.((jpg)|(png))\")\n",
    "\n",
    "filename_to_id = {root + \"/\" + file: id_from_filename(file) for file in file_names}\n",
    "id_to_filename = {v:k for k,v in filename_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24db50b-73a4-4834-a8ed-06e3146c696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.3.bias', 'classifier.0.weight', 'classifier.1.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViltForQuestionAnswering(\n",
       "  (vilt): ViltModel(\n",
       "    (embeddings): ViltEmbeddings(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768)\n",
       "        (position_embeddings): Embedding(40, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (patch_embeddings): ViltPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViltEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViltPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1536, out_features=3129, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",\n",
    "                                                 id2label=config.id2label,\n",
    "                                                 label2id=config.label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3f2210-6638-4c74-a5e3-2c11973171bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questions\n",
    "data_questions = json.load(open(path_questions))\n",
    "questions = data_questions['questions']\n",
    "\n",
    "#Annotations\n",
    "data_annotations = json.load(open(path_annotations))\n",
    "annotations = data_annotations['annotations']\n",
    "\n",
    "#Processor\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "\n",
    "#Dataset\n",
    "dataset = VQADataset(questions=questions[:100],\n",
    "                     annotations=annotations[:100],\n",
    "                     processor=processor)\n",
    "\n",
    "#Train dataloader\n",
    "train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657a0ec0-f863-4496-9196-61654dcef7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51d54fd96f645c886c3872cdb423c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for annotation in tqdm(annotations):\n",
    "    answers = annotation['answers']\n",
    "    answer_count = {}\n",
    "    for answer in answers:\n",
    "        answer_ = answer[\"answer\"]\n",
    "        answer_count[answer_] = answer_count.get(answer_, 0) + 1\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for answer in answer_count:\n",
    "        if answer not in list(config.label2id.keys()):\n",
    "            continue\n",
    "        labels.append(config.label2id[answer])\n",
    "        score = get_score(answer_count[answer])\n",
    "        scores.append(score)\n",
    "    annotation['labels'] = labels\n",
    "    annotation['scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68426e10-9248-4dad-b02f-692eea275fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2fb72dd02e41239e5eb7370b52035c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 956.8610229492188\n",
      "Loss: 921.6065063476562\n",
      "Loss: 888.2288208007812\n",
      "Loss: 855.6438598632812\n",
      "Loss: 822.6932373046875\n",
      "Loss: 793.51806640625\n",
      "Loss: 766.9517822265625\n",
      "Loss: 739.0214233398438\n",
      "Loss: 712.250244140625\n",
      "Loss: 689.1754150390625\n",
      "Loss: 663.0001220703125\n",
      "Loss: 639.6845092773438\n",
      "Loss: 617.6478881835938\n",
      "Loss: 595.3203735351562\n",
      "Loss: 574.157958984375\n",
      "Loss: 554.617431640625\n",
      "Loss: 535.9575805664062\n",
      "Loss: 517.53857421875\n",
      "Loss: 500.2691345214844\n",
      "Loss: 482.7394714355469\n",
      "Loss: 466.7906799316406\n",
      "Loss: 450.4107971191406\n",
      "Loss: 435.88555908203125\n",
      "Loss: 421.92926025390625\n",
      "Loss: 408.8621826171875\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa13a0ba050499e8472169194c0a435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 396.33319091796875\n",
      "Loss: 381.93218994140625\n",
      "Loss: 370.30377197265625\n",
      "Loss: 360.6387634277344\n",
      "Loss: 348.5657043457031\n",
      "Loss: 336.47064208984375\n",
      "Loss: 328.6730651855469\n",
      "Loss: 316.94097900390625\n",
      "Loss: 307.144775390625\n",
      "Loss: 298.4525451660156\n",
      "Loss: 289.9792785644531\n",
      "Loss: 283.0616455078125\n",
      "Loss: 274.0792236328125\n",
      "Loss: 265.34515380859375\n",
      "Loss: 259.7699890136719\n",
      "Loss: 253.74591064453125\n",
      "Loss: 245.09278869628906\n",
      "Loss: 238.89111328125\n",
      "Loss: 234.4183349609375\n",
      "Loss: 228.06741333007812\n",
      "Loss: 221.37376403808594\n",
      "Loss: 217.68740844726562\n",
      "Loss: 210.84178161621094\n",
      "Loss: 205.9726104736328\n",
      "Loss: 201.2008819580078\n",
      "Training execution time (mins) 0.855648132165273\n",
      "Training Job Complete\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):  # loop over the dataset multiple times, trainign only 10 epochs\n",
    "   print(f\"Epoch: {epoch}\")\n",
    "   for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(\"Loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "execution_time = (time.time() - start_time) / 60.0\n",
    "print(\"Training execution time (mins)\", execution_time)\n",
    "print(\"Training Job Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d2c27-dc6b-436a-a518-faca620fc212",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9050ae34-45ea-4162-b8b2-60a5daac9942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] where is he looking? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[0]\n",
    "processor.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0667871-e20d-483f-baaa-4bc3a0d22dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2725496292114258 no\n",
      "0.23888686299324036 yes\n",
      "0.23368898034095764 tie\n",
      "0.20375598967075348 scarf\n",
      "0.19615337252616882 kites\n"
     ]
    }
   ],
   "source": [
    "# add batch dimension + move to GPU\n",
    "example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n",
    "# forward pass\n",
    "outputs = model(**example)\n",
    "\n",
    "logits = outputs.logits\n",
    "predicted_classes = torch.sigmoid(logits)\n",
    "\n",
    "probs, classes = torch.topk(predicted_classes, 5)\n",
    "\n",
    "for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):\n",
    "  print(prob, model.config.id2label[class_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
