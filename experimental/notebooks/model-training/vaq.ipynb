{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models for visual to question inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaVUyM5wzWgE",
        "outputId": "5d8ae6a8-ff8d-43f2-8927-1deaf481ec71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GQ3mDxKHp601"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def upload_and_validate_image():\n",
        "    try:\n",
        "        # Prompt the user to upload an image file\n",
        "        image_path = input(\"Please enter the path to the image file: \")\n",
        "\n",
        "        # Open the image using PIL\n",
        "        with Image.open(image_path) as img:\n",
        "            # Check if the image format is either JPG or PNG\n",
        "            if img.format in (\"JPEG\", \"PNG\"):\n",
        "                print(f\"Image is valid. It is in {img.format} format.\")\n",
        "                return img\n",
        "            else:\n",
        "                print(\"Invalid image format. Please upload a JPG or PNG image.\")\n",
        "                return None\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Please provide a valid file path.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAB7r9gMtxu7",
        "outputId": "c66734e0-0094-4a4b-f243-082f26a39577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter the path to the image file: /content/pandas.jpeg\n",
            "Image is valid. It is in JPEG format.\n"
          ]
        }
      ],
      "source": [
        "uploaded_image = upload_and_validate_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmrwGN9X4ckC"
      },
      "source": [
        "in case the path does not work, please refer to upload to the link of your image as per the case below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jK-CmDg-2__a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHSOXElcvaSe"
      },
      "source": [
        "# Captioner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTSF4pWyvb7H"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "def model_1(image):\n",
        "    captioner = pipeline(model=\"ydshieh/vit-gpt2-coco-en\")\n",
        "    return captioner(image)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZrPTYXVv1i9"
      },
      "source": [
        "# Closed answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "th8qLGefv4BZ"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "def model_2(image, text):\n",
        "#image = Image.open(requests.get(url, stream=True).raw)\n",
        "    processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "    model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "    encoding = processor(image, text, return_tensors=\"pt\")\n",
        "    outputs = model(**encoding)\n",
        "    logits = outputs.logits\n",
        "    idx = logits.argmax(-1).item()\n",
        "    print(\"Predicted answer:\", model.config.id2label[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "29DrNVadwJ5Z"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fabca03213224ceba4204c7ab2210b49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)rocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dc735d5af944b45a9796d957eee93a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13170f857dfe45659d2fc25badca887b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81ac635bffca43c78880fa95903978f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cee22a8d648b42edadeedc634d8fe4db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55e5513a6e424503b82531124df3eca2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c42d95c8843424f90cbc86e3fd4f78b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/470M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted answer: 2\n"
          ]
        }
      ],
      "source": [
        "output_2 = model_2(image, text = \"How many cats are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3Xv7iKuvwdwz"
      },
      "outputs": [],
      "source": [
        "output_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTJx6owwvkwz"
      },
      "source": [
        "# Zero Shot - SOTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blkeYefnvlsL"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "# import torch\n",
        "\n",
        "# def model_3(image,text):\n",
        "#   processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "#   model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "#   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#   model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7WChqD4vnXE"
      },
      "outputs": [],
      "source": [
        "# question = \"what is on the image\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHis0EVBvngX"
      },
      "outputs": [],
      "source": [
        "# prompt = f\"Question: {question} Answer:\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vilt b32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5MN-8nVTzyex"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "# import requests\n",
        "# from PIL import Image\n",
        "\n",
        "\n",
        "# pipe = pipeline(\"fill-mask\", model=\"dandelin/vilt-b32-mlm\")\n",
        "# image_url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png\"\n",
        "# pipe(question=\"What is she wearing ?\", inputs=image_url)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
